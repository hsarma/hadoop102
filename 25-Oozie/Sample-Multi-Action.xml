<workflow-app xmlns='uri:oozie:workflow:0.1' name='demo-wf'>
  <start to="map_reduce_1" />
  <action name="map_reduce_1">
    <map-reduce>
      <job-tracker>${jobTracker}</job-tracker>
      <name-node>${nameNode}</name-node>
      <configuration>
        <property>
          <name>mapred.mapper.class</name>
          <value>org.apache.oozie.example.DemoMapper</value>
        </property>
        <property>
            <name>mapred.mapoutput.key.class</name>
            <value>org.apache.hadoop.io.Text</value>
        </property>
        <property>
            <name>mapred.mapoutput.value.class</name>
            <value>org.apache.hadoop.io.IntWritable</value>
        </property>
        <property>
          <name>mapred.reducer.class</name>
          <value>org.apache.oozie.example.DemoReducer</value>
        </property>
        <property>
          <name>mapred.map.tasks</name>
          <value>1</value>
        </property>
        <property>
          <name>mapred.input.dir</name>
          <value>${inputDir}</value>
        </property>
        <property>
          <name>mapred.output.dir</name>
          <value>${outputDir}/mapred_1</value>
        </property>
        <property>
          <name>mapred.job.queue.name</name>
          <value>${queueName}</value>
        </property>
      </configuration>
    </map-reduce>
    <ok to="fork_1" />
    <error to="fail_1" />
  </action>
  <fork name='fork_1'>
        <path start='hdfs_1' />
        <path start='hadoop_streaming_1' />
  </fork>
  <action name="hdfs_1">
    <fs>
      <mkdir path="${nameNode}/tmp/${wf:user()}/hdfsdir1" />
    </fs>
    <ok to="join_1" />
    <error to="fail_1" />
  </action>
  <action name="hadoop_streaming_1">
  <map-reduce>
      <job-tracker>${jobTracker}</job-tracker>
      <name-node>${nameNode}</name-node>
      <prepare>
        <delete path="${nameNode}/tmp/${wf:user()}/hdfsdir1" />
      </prepare>
      <streaming>
        <mapper>/bin/cat</mapper>
        <reducer>/usr/bin/wc</reducer>
      </streaming>
      <configuration>
        <property>
          <name>mapred.input.dir</name>
          <value>${outputDir}/mapred_1</value>
        </property>
        <property>
          <name>mapred.output.dir</name>
          <value>${outputDir}/streaming</value>
        </property>
      </configuration>
    </map-reduce>
    <ok to="join_1" />
    <error to="fail_1" />
  </action>
  <join name='join_1' to='pig_1' />
   <action name="pig_1">
    <pig>
        <job-tracker>${jobTracker}</job-tracker>
        <name-node>${nameNode}</name-node>
        <configuration>
            <property>
                <name>mapred.map.output.compress</name>
                <value>false</value>
            </property>
            <property>
              <name>mapred.job.queue.name</name>
              <value>${queueName}</value>
            </property>
        </configuration>
        <script>org/apache/oozie/examples/pig/id.pig</script>
        <param>INPUT=${outputDir}/mapred_1</param>
        <param>OUTPUT=${outputDir}/pig_1</param>
    </pig>
    <ok to="end_1" />
    <error to="fail_1" />
  </action>
  <kill name="fail_1">
   <message>Demo workflow failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
 </kill>
 <end name="end_1" />
</workflow-app>